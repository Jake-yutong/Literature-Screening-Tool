#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Literature Screening Tool for Meta-Analysis / Bibliometrics
æ–‡çŒ®ç²—ç­›å·¥å…· - ç”¨äºå…ƒåˆ†æ/æ–‡çŒ®è®¡é‡å­¦ç ”ç©¶

Author: Auto-generated for research purposes
Usage: python literature_screener.py

This script automates the initial screening process for systematic reviews
by filtering literature based on title, abstract, and journal keywords.
"""

import pandas as pd
import os
import sys
from datetime import datetime
from pathlib import Path


# ============================================================================
# ğŸ”§ CONFIGURATION SECTION - é…ç½®åŒºåŸŸ
# Modify these lists according to your research needs
# æ ¹æ®ä½ çš„ç ”ç©¶éœ€æ±‚ä¿®æ”¹è¿™äº›åˆ—è¡¨
# ============================================================================

# ğŸ“‹ Keywords to EXCLUDE in Title or Abstract (case-insensitive)
# åœ¨æ ‡é¢˜æˆ–æ‘˜è¦ä¸­éœ€è¦æ’é™¤çš„å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
TITLE_ABSTRACT_BLACKLIST = [
    # Medical/Clinical terms - åŒ»å­¦/ä¸´åºŠæœ¯è¯­
    "surgical", "surgery", "patient", "patients", "clinical trial",
    "hospital", "physician", "nurse", "disease", "therapy", "therapeutic",
    "diagnosis", "treatment", "medication", "drug", "pharmaceutical",
    "pathology", "symptom", "syndrome", "cancer", "tumor", "tumour",
    
    # Sports/Physical terms - ä½“è‚²/è¿åŠ¨æœ¯è¯­
    "athlete", "athletes", "sports", "swimming", "football", "basketball",
    "soccer", "marathon", "Olympic", "championship",
    
    # Game theory & unrelated fields - åšå¼ˆè®ºåŠä¸ç›¸å…³é¢†åŸŸ
    "game theory", "game-theoretic", "poker", "chess", "video game",
    
    # Chemistry/Physics terms - åŒ–å­¦/ç‰©ç†æœ¯è¯­
    "molecular", "molecule", "chemical", "chemistry", "physics",
    "quantum", "atomic", "electron", "polymer", "catalyst",
    
    # Biology/Life sciences - ç”Ÿç‰©/ç”Ÿå‘½ç§‘å­¦
    "genome", "genomic", "protein", "enzyme", "bacteria", "virus",
    "cell culture", "in vitro", "in vivo", "rodent", "mice", "rats",
    
    # Add your own keywords below - åœ¨ä¸‹æ–¹æ·»åŠ ä½ è‡ªå·±çš„å…³é”®è¯
    # "your_keyword_here",
]

# ğŸ“° Keywords to EXCLUDE in Source Title / Journal Name (case-insensitive)
# åœ¨æœŸåˆŠåç§°ä¸­éœ€è¦æ’é™¤çš„å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
JOURNAL_BLACKLIST = [
    # Medical journals - åŒ»å­¦æœŸåˆŠ
    "medicine", "medical", "clinical", "surgery", "surgical",
    "hospital", "health", "nursing", "pharmacy", "pharmacology",
    "oncology", "cardiology", "neurology", "psychiatry", "pediatric",
    
    # Chemistry/Physics journals - åŒ–å­¦/ç‰©ç†æœŸåˆŠ
    "chemistry", "chemical", "physics", "physical",
    
    # Biology journals - ç”Ÿç‰©æœŸåˆŠ
    "biology", "biological", "biochemistry", "microbiology",
    "genetics", "genomics", "molecular", "cell",
    
    # Sports journals - ä½“è‚²æœŸåˆŠ
    "sports", "sport", "athletic", "exercise", "physical education",
    
    # Add your own journal keywords below - åœ¨ä¸‹æ–¹æ·»åŠ ä½ è‡ªå·±çš„æœŸåˆŠå…³é”®è¯
    # "your_journal_keyword_here",
]

# ğŸ“Š Column name mappings (adjust based on your data source)
# åˆ—åæ˜ å°„ï¼ˆæ ¹æ®ä½ çš„æ•°æ®æ¥æºè°ƒæ•´ï¼‰
# Supports: Web of Science, Scopus, and common export formats
COLUMN_MAPPINGS = {
    # Title columns
    "title": ["Title", "title", "TI", "Article Title", "Document Title"],
    # Abstract columns  
    "abstract": ["Abstract", "abstract", "AB", "Description"],
    # Source/Journal columns
    "source": ["Source Title", "source title", "SO", "Source", "Journal", 
               "Publication Name", "Publication", "Journal Title"],
}


# ============================================================================
# ğŸš€ MAIN SCREENING LOGIC - ä¸»ç­›é€‰é€»è¾‘
# ============================================================================

def find_column(df: pd.DataFrame, column_type: str) -> str | None:
    """Find the actual column name in the dataframe based on mappings."""
    possible_names = COLUMN_MAPPINGS.get(column_type, [])
    for name in possible_names:
        if name in df.columns:
            return name
    return None


def contains_blacklisted_keyword(text: str, blacklist: list) -> tuple[bool, str]:
    """
    Check if text contains any blacklisted keyword.
    Returns (is_blacklisted, matched_keyword).
    """
    if pd.isna(text) or not isinstance(text, str):
        return False, ""
    
    text_lower = text.lower()
    for keyword in blacklist:
        if keyword.lower() in text_lower:
            return True, keyword
    return False, ""


def screen_literature(input_file: str, output_dir: str = None) -> dict:
    """
    Main screening function.
    
    Args:
        input_file: Path to the input Excel/CSV file
        output_dir: Directory for output files (defaults to input file's directory)
    
    Returns:
        Dictionary with screening statistics
    """
    print("\n" + "=" * 60)
    print("ğŸ“š LITERATURE SCREENING TOOL / æ–‡çŒ®ç­›é€‰å·¥å…·")
    print("=" * 60)
    
    # Determine file type and read data
    input_path = Path(input_file)
    if not input_path.exists():
        print(f"âŒ Error: File not found - {input_file}")
        sys.exit(1)
    
    print(f"\nğŸ“‚ Reading file: {input_path.name}")
    
    if input_path.suffix.lower() in ['.xlsx', '.xls']:
        df = pd.read_excel(input_file, engine='openpyxl')
    elif input_path.suffix.lower() == '.csv':
        # Try different encodings
        for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'latin-1']:
            try:
                df = pd.read_csv(input_file, encoding=encoding)
                break
            except UnicodeDecodeError:
                continue
        else:
            print("âŒ Error: Could not decode CSV file with common encodings")
            sys.exit(1)
    else:
        print(f"âŒ Error: Unsupported file format - {input_path.suffix}")
        print("   Supported formats: .xlsx, .xls, .csv")
        sys.exit(1)
    
    total_records = len(df)
    print(f"   Total records loaded: {total_records:,}")
    print(f"   Columns detected: {list(df.columns)[:5]}...")
    
    # Find relevant columns
    title_col = find_column(df, "title")
    abstract_col = find_column(df, "abstract")
    source_col = find_column(df, "source")
    
    print(f"\nğŸ” Column Detection:")
    print(f"   Title column: {title_col or 'âŒ Not found'}")
    print(f"   Abstract column: {abstract_col or 'âŒ Not found'}")
    print(f"   Source/Journal column: {source_col or 'âŒ Not found'}")
    
    if not title_col:
        print("\nâš ï¸  Warning: No title column found. Screening accuracy may be reduced.")
    
    # Initialize tracking columns
    df['_EXCLUDED'] = False
    df['_EXCLUSION_REASON'] = ''
    
    # Screening process
    print(f"\nâ³ Screening in progress...")
    
    excluded_count = 0
    title_abstract_excluded = 0
    journal_excluded = 0
    
    for idx, row in df.iterrows():
        exclusion_reasons = []
        
        # Check Title
        if title_col:
            is_excluded, keyword = contains_blacklisted_keyword(
                row[title_col], TITLE_ABSTRACT_BLACKLIST
            )
            if is_excluded:
                exclusion_reasons.append(f"Title contains: '{keyword}'")
        
        # Check Abstract
        if abstract_col:
            is_excluded, keyword = contains_blacklisted_keyword(
                row[abstract_col], TITLE_ABSTRACT_BLACKLIST
            )
            if is_excluded:
                exclusion_reasons.append(f"Abstract contains: '{keyword}'")
        
        # Count title/abstract exclusions
        if exclusion_reasons:
            title_abstract_excluded += 1
        
        # Check Source/Journal
        if source_col:
            is_excluded, keyword = contains_blacklisted_keyword(
                row[source_col], JOURNAL_BLACKLIST
            )
            if is_excluded:
                exclusion_reasons.append(f"Journal contains: '{keyword}'")
                if len(exclusion_reasons) == 1:  # Only journal exclusion
                    journal_excluded += 1
        
        # Mark as excluded if any reason found
        if exclusion_reasons:
            df.at[idx, '_EXCLUDED'] = True
            df.at[idx, '_EXCLUSION_REASON'] = ' | '.join(exclusion_reasons)
            excluded_count += 1
    
    # Split into kept and removed dataframes
    df_kept = df[df['_EXCLUDED'] == False].drop(columns=['_EXCLUDED', '_EXCLUSION_REASON'])
    df_removed = df[df['_EXCLUDED'] == True].copy()
    df_removed = df_removed.rename(columns={'_EXCLUSION_REASON': 'Exclusion_Reason'})
    df_removed = df_removed.drop(columns=['_EXCLUDED'])
    
    # Output setup
    if output_dir is None:
        output_dir = input_path.parent
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save cleaned data
    cleaned_file = output_dir / f"cleaned_data_{timestamp}.csv"
    df_kept.to_csv(cleaned_file, index=False, encoding='utf-8-sig')
    
    # Save removed data
    removed_file = output_dir / f"removed_data_{timestamp}.csv"
    df_removed.to_csv(removed_file, index=False, encoding='utf-8-sig')
    
    # Calculate statistics
    kept_count = len(df_kept)
    kept_percentage = (kept_count / total_records * 100) if total_records > 0 else 0
    
    # Print results
    print("\n" + "=" * 60)
    print("ğŸ“Š SCREENING RESULTS / ç­›é€‰ç»“æœ")
    print("=" * 60)
    print(f"""
    ğŸ“¥ Input records:           {total_records:,}
    âœ… Retained (clean):        {kept_count:,} ({kept_percentage:.1f}%)
    âŒ Excluded (removed):      {excluded_count:,} ({100-kept_percentage:.1f}%)
    
    Exclusion breakdown:
    â”œâ”€ Title/Abstract keywords: {title_abstract_excluded:,}
    â””â”€ Journal name keywords:   {journal_excluded:,}
    
    ğŸ“ Output files:
    â”œâ”€ {cleaned_file.name}
    â””â”€ {removed_file.name}
    
    ğŸ“ Output directory: {output_dir}
    """)
    
    print("=" * 60)
    print("âœ¨ Screening complete! / ç­›é€‰å®Œæˆï¼")
    print("=" * 60)
    
    # Tips
    print("""
ğŸ’¡ NEXT STEPS / åç»­æ­¥éª¤:
   1. Check 'removed_data_*.csv' to verify no important papers were excluded
      æ£€æŸ¥ removed_data æ–‡ä»¶ï¼Œç¡®è®¤æ²¡æœ‰è¯¯åˆ é‡è¦æ–‡çŒ®
   
   2. Import 'cleaned_data_*.csv' into VOSviewer for analysis
      å°† cleaned_data å¯¼å…¥ VOSviewer è¿›è¡Œåˆ†æ
   
   3. Use the exclusion data for PRISMA flow diagram
      ä½¿ç”¨æ’é™¤æ•°æ®ç»˜åˆ¶ PRISMA æµç¨‹å›¾
    """)
    
    return {
        'total': total_records,
        'kept': kept_count,
        'excluded': excluded_count,
        'cleaned_file': str(cleaned_file),
        'removed_file': str(removed_file),
    }


# ============================================================================
# ğŸ¯ ENTRY POINT - ç¨‹åºå…¥å£
# ============================================================================

def main():
    """Main entry point with interactive mode."""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ“š Literature Screening Tool for Meta-Analysis          â•‘
    â•‘   æ–‡çŒ®ç²—ç­›å·¥å…· - å…ƒåˆ†æ/æ–‡çŒ®è®¡é‡å­¦ç ”ç©¶                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Check for command line argument
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        # Interactive mode
        print("Please enter the path to your literature file (Excel or CSV):")
        print("è¯·è¾“å…¥æ–‡çŒ®æ–‡ä»¶çš„è·¯å¾„ï¼ˆæ”¯æŒ Excel æˆ– CSVï¼‰:\n")
        input_file = input(">>> ").strip().strip('"').strip("'")
    
    if not input_file:
        print("âŒ No file provided. Exiting.")
        sys.exit(1)
    
    # Run screening
    results = screen_literature(input_file)
    
    print("\nğŸ‰ All done! Press Enter to exit...")
    input()


if __name__ == "__main__":
    main()
